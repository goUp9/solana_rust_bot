2026-01-10 01:24:43 | INFO | game_rl_training | Loading tokenizer: Qwen/Qwen3-4B
2026-01-10 01:24:45 | INFO | game_rl_training | Loading base model: Qwen/Qwen3-4B
`torch_dtype` is deprecated! Use `dtype` instead!
2026-01-10 01:24:47 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.25s/it]
2026-01-10 01:24:54 | INFO | game_rl_training | Applying LoRA configuration...
2026-01-10 01:24:54 | WARNING | root | The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.
2026-01-10 01:24:55 | INFO | root | peft adapter initialised
2026-01-10 01:24:55 | INFO | game_rl_training | Creating reference model...
2026-01-10 01:24:55 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.24s/it]
2026-01-10 01:25:03 | WARNING | root | A <class 'transformers.models.qwen3.modeling_qwen3.Qwen3ForCausalLM'> model is loaded from 'Qwen/Qwen3-4B', and no v_head weight is found. This IS expected if you are not resuming PPO training.
2026-01-10 01:25:03 | INFO | game_rl_training | Initializing PPO trainer...
wandb: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
/root/workplace/game_rl_training/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:262: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.
  warnings.warn(
2026-01-10 01:25:04 | INFO | game_rl_training | Using local OpenSpiel execution (in-process inference, no env server, no vLLM).
2026-01-10 01:25:04 | INFO | game_rl_training | Initializing curriculum sampler...
2026-01-10 01:25:04 | INFO | game_rl_training | Setup complete!
2026-01-10 01:25:04 | INFO | game_rl_training | Starting training...
2026-01-10 01:25:04 | INFO | game_rl_training | Step 1/500: Collecting rollouts from 16 episodes...
2026-01-10 01:28:22 | INFO | game_rl_training | Training with 9 samples...
2026-01-10 01:28:25 | INFO | game_rl_training | Step 1 | Reward: 0.3322 | Score: 0.3333 | Success: 33.33% | ValidRate: 9.10%
2026-01-10 01:28:25 | INFO | game_rl_training | Step 2/500: Collecting rollouts from 16 episodes...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Caching is incompatible with gradient checkpointing in Qwen3DecoderLayer. Setting `past_key_values=None`.
/root/workplace/game_rl_training/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/root/workplace/game_rl_training/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
2026-01-10 01:32:55 | WARNING | game_rl_training | Step 2: Only 2 valid samples collected (need 4). Skipping training step.
2026-01-10 01:32:55 | INFO | game_rl_training | Step 3/500: Collecting rollouts from 16 episodes...
2026-01-10 01:35:21 | INFO | game_rl_training | Training with 5 samples...
2026-01-10 01:35:22 | INFO | game_rl_training | Step 3 | Reward: 0.3980 | Score: 0.4000 | Success: 40.00% | ValidRate: 2.97%
2026-01-10 01:35:22 | INFO | game_rl_training | Step 4/500: Collecting rollouts from 16 episodes...
2026-01-10 01:37:42 | WARNING | game_rl_training | Step 4: Only 3 valid samples collected (need 4). Skipping training step.
2026-01-10 01:37:42 | INFO | game_rl_training | Step 5/500: Collecting rollouts from 16 episodes...
2026-01-10 01:43:55 | INFO | game_rl_training | Training with 6 samples...
2026-01-10 01:43:57 | INFO | game_rl_training | Step 5 | Reward: 1.0000 | Score: 1.0000 | Success: 100.00% | ValidRate: 3.45%
2026-01-10 01:43:57 | INFO | game_rl_training | Step 6/500: Collecting rollouts from 16 episodes...
2026-01-10 01:47:44 | WARNING | game_rl_training | Step 6: Only 2 valid samples collected (need 4). Skipping training step.
2026-01-10 01:47:44 | INFO | game_rl_training | Step 7/500: Collecting rollouts from 16 episodes...
2026-01-10 01:52:53 | WARNING | game_rl_training | Step 7: Only 3 valid samples collected (need 4). Skipping training step.
2026-01-10 01:52:53 | INFO | game_rl_training | Step 8/500: Collecting rollouts from 16 episodes...
2026-01-10 01:54:58 | INFO | game_rl_training | Training with 9 samples...
2026-01-10 01:55:01 | INFO | game_rl_training | Step 8 | Reward: 0.3322 | Score: 0.3333 | Success: 33.33% | ValidRate: 6.62%
2026-01-10 01:55:01 | INFO | game_rl_training | Step 9/500: Collecting rollouts from 16 episodes...
2026-01-10 01:58:03 | INFO | game_rl_training | Training with 4 samples...
2026-01-10 01:58:04 | INFO | game_rl_training | Step 9 | Reward: 0.5000 | Score: 0.5000 | Success: 50.00% | ValidRate: 6.67%
2026-01-10 01:58:04 | INFO | game_rl_training | Step 10/500: Collecting rollouts from 16 episodes...
2026-01-10 01:59:31 | INFO | game_rl_training | Training with 4 samples...
2026-01-10 01:59:33 | INFO | game_rl_training | Step 10 | Reward: 0.5000 | Score: 0.5000 | Success: 50.00% | ValidRate: 6.77%
2026-01-10 01:59:33 | INFO | game_rl_training | Step 11/500: Collecting rollouts from 16 episodes...
2026-01-10 02:02:22 | WARNING | game_rl_training | Step 11: Only 2 valid samples collected (need 4). Skipping training step.
2026-01-10 02:02:22 | INFO | game_rl_training | Step 12/500: Collecting rollouts from 16 episodes...
2026-01-10 02:06:27 | WARNING | game_rl_training | Step 12: Only 2 valid samples collected (need 4). Skipping training step.
2026-01-10 02:06:27 | INFO | game_rl_training | Step 13/500: Collecting rollouts from 16 episodes...
2026-01-10 02:11:32 | WARNING | game_rl_training | Step 13: Only 3 valid samples collected (need 4). Skipping training step.
2026-01-10 02:11:32 | INFO | game_rl_training | Step 14/500: Collecting rollouts from 16 episodes...
2026-01-10 02:14:13 | INFO | game_rl_training | Training with 4 samples...
2026-01-10 02:14:14 | INFO | game_rl_training | Step 14 | Reward: 0.2500 | Score: 0.2500 | Success: 25.00% | ValidRate: 3.29%
2026-01-10 02:14:14 | INFO | game_rl_training | Step 15/500: Collecting rollouts from 16 episodes...
2026-01-10 02:19:06 | WARNING | game_rl_training | Step 15: Only 3 valid samples collected (need 4). Skipping training step.
2026-01-10 02:19:06 | INFO | game_rl_training | Step 16/500: Collecting rollouts from 16 episodes...
2026-01-10 02:22:19 | INFO | game_rl_training | Training with 7 samples...
2026-01-10 02:22:21 | INFO | game_rl_training | Step 16 | Reward: 0.7143 | Score: 0.7143 | Success: 71.43% | ValidRate: 9.56%
2026-01-10 02:22:21 | INFO | game_rl_training | Step 17/500: Collecting rollouts from 16 episodes...
2026-01-10 02:25:03 | INFO | game_rl_training | Training with 4 samples...
2026-01-10 02:25:05 | INFO | game_rl_training | Step 17 | Reward: 0.2500 | Score: 0.2500 | Success: 25.00% | ValidRate: 4.94%
2026-01-10 02:25:05 | INFO | game_rl_training | Step 18/500: Collecting rollouts from 16 episodes...
