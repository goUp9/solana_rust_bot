2026-01-10 00:25:32 | INFO | game_rl_training | Loading tokenizer: Qwen/Qwen3-4B
2026-01-10 00:25:33 | INFO | game_rl_training | Loading base model: Qwen/Qwen3-4B
`torch_dtype` is deprecated! Use `dtype` instead!
2026-01-10 00:25:36 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.28s/it]
2026-01-10 00:25:43 | INFO | game_rl_training | Applying LoRA configuration...
2026-01-10 00:25:43 | WARNING | root | The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.
2026-01-10 00:25:44 | INFO | root | peft adapter initialised
2026-01-10 00:25:44 | INFO | game_rl_training | Creating reference model...
2026-01-10 00:25:44 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.25s/it]
2026-01-10 00:25:51 | WARNING | root | A <class 'transformers.models.qwen3.modeling_qwen3.Qwen3ForCausalLM'> model is loaded from 'Qwen/Qwen3-4B', and no v_head weight is found. This IS expected if you are not resuming PPO training.
2026-01-10 00:25:52 | INFO | game_rl_training | Initializing PPO trainer...
wandb: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
/root/workplace/game_rl_training/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:262: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.
  warnings.warn(
2026-01-10 00:25:52 | INFO | game_rl_training | Using local OpenSpiel execution (in-process inference, no env server, no vLLM).
2026-01-10 00:25:52 | INFO | game_rl_training | Initializing curriculum sampler...
2026-01-10 00:25:52 | INFO | game_rl_training | Setup complete!
2026-01-10 00:25:52 | INFO | game_rl_training | Starting training...
2026-01-10 00:25:52 | INFO | game_rl_training | Step 1/500: Collecting rollouts from 16 episodes...
2026-01-10 00:42:13 | INFO | game_rl_training | Training with 12 samples...
2026-01-10 00:42:22 | INFO | game_rl_training | Step 2/500: Collecting rollouts from 16 episodes...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Caching is incompatible with gradient checkpointing in Qwen3DecoderLayer. Setting `past_key_values=None`.
/root/workplace/game_rl_training/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/root/workplace/game_rl_training/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
2026-01-10 00:56:31 | INFO | game_rl_training | Training with 8 samples...
/root/workplace/game_rl_training/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.
  warnings.warn(
2026-01-10 00:56:37 | INFO | game_rl_training | Step 3/500: Collecting rollouts from 16 episodes...
2026-01-10 01:10:51 | INFO | game_rl_training | Training with 6 samples...
/root/workplace/game_rl_training/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.
  warnings.warn(
2026-01-10 01:10:54 | INFO | game_rl_training | Step 4/500: Collecting rollouts from 16 episodes...
2026-01-10 01:19:01 | INFO | game_rl_training | Training with 5 samples...
/root/workplace/game_rl_training/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.36 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.
  warnings.warn(
2026-01-10 01:19:04 | INFO | game_rl_training | Step 5/500: Collecting rollouts from 16 episodes...
