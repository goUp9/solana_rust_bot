2026-01-19 09:14:38 | INFO | game_rl_training | Setting up PPO+LoRA training...
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: tech-whiz0314 (tech-whiz0314-beta) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run e6aq12sa
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /root/workspace/game_rl_training/wandb/run-20260119_091438-e6aq12sa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-fog-77
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tech-whiz0314-beta/game-rl-training
wandb: üöÄ View run at https://wandb.ai/tech-whiz0314-beta/game-rl-training/runs/e6aq12sa
2026-01-19 09:14:39 | INFO | game_rl_training | Loading tokenizer: Qwen/Qwen3-4B-Instruct-2507
2026-01-19 09:14:40 | INFO | game_rl_training | Loading base model: Qwen/Qwen3-4B-Instruct-2507
`torch_dtype` is deprecated! Use `dtype` instead!
2026-01-19 09:14:43 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:02<00:04,  2.08s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:04<00:02,  2.29s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.53s/it]
2026-01-19 09:14:48 | INFO | game_rl_training | Applying LoRA configuration...
2026-01-19 09:14:48 | WARNING | root | The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.
2026-01-19 09:14:49 | INFO | root | peft adapter initialised
2026-01-19 09:14:49 | INFO | game_rl_training | Creating reference model...
2026-01-19 09:14:49 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:02<00:04,  2.04s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:04<00:02,  2.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.52s/it]
2026-01-19 09:14:54 | WARNING | root | A <class 'transformers.models.qwen3.modeling_qwen3.Qwen3ForCausalLM'> model is loaded from 'Qwen/Qwen3-4B-Instruct-2507', and no v_head weight is found. This IS expected if you are not resuming PPO training.
2026-01-19 09:14:55 | INFO | game_rl_training | Initializing PPO trainer...
wandb: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
/root/workspace/game_rl_training/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:262: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.
  warnings.warn(
2026-01-19 09:14:55 | INFO | game_rl_training | Using local OpenSpiel execution (in-process inference, no env server, no vLLM).
2026-01-19 09:14:55 | INFO | game_rl_training | Initializing curriculum sampler...
2026-01-19 09:14:55 | INFO | game_rl_training | Setup complete!
2026-01-19 09:14:55 | INFO | game_rl_training | Running SFT warmup for 100 steps...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/root/workspace/game_rl_training/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
2026-01-19 09:15:00 | INFO | game_rl_training | SFT step 10/100 | Loss: 3.0276
2026-01-19 09:15:04 | INFO | game_rl_training | SFT step 20/100 | Loss: 3.6384
2026-01-19 09:15:07 | INFO | game_rl_training | SFT step 30/100 | Loss: 3.1375
2026-01-19 09:15:11 | INFO | game_rl_training | SFT step 40/100 | Loss: 2.9358
2026-01-19 09:15:14 | INFO | game_rl_training | SFT step 50/100 | Loss: 2.6351
2026-01-19 09:15:19 | INFO | game_rl_training | SFT step 60/100 | Loss: 2.4571
2026-01-19 09:15:22 | INFO | game_rl_training | SFT step 70/100 | Loss: 2.2320
2026-01-19 09:15:27 | INFO | game_rl_training | SFT step 80/100 | Loss: 2.0139
2026-01-19 09:15:34 | INFO | game_rl_training | SFT step 100/100 | Loss: 2.0421
2026-01-19 09:15:34 | INFO | game_rl_training | SFT warmup complete. Avg loss: 2.0421
2026-01-19 09:15:34 | INFO | game_rl_training | Starting PPO training...
2026-01-19 09:15:34 | INFO | game_rl_training | Step 1/1000: Collecting rollouts from 16 episodes...
Caching is incompatible with gradient checkpointing in Qwen3DecoderLayer. Setting `past_key_values=None`.
/root/workspace/game_rl_training/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
2026-01-19 09:16:41 | INFO | game_rl_training | Collected 16 valid samples from 36 episodes. Games with valid samples: {'liars_dice': 16}
2026-01-19 09:16:41 | INFO | game_rl_training | Training with 16 samples...
2026-01-19 09:16:47 | INFO | game_rl_training | Step 1 | Reward: 0.5625 | Score: 0.5625 | Success: 56.25% | ValidRate: 24.52%
2026-01-19 09:16:47 | INFO | game_rl_training | Step 2/1000: Collecting rollouts from 16 episodes...
2026-01-19 09:17:46 | INFO | game_rl_training | Collected 16 valid samples from 36 episodes. Games with valid samples: {'liars_dice': 16}
2026-01-19 09:17:46 | INFO | game_rl_training | Training with 16 samples...
2026-01-19 09:17:52 | INFO | game_rl_training | Step 2 | Reward: 0.3125 | Score: 0.3125 | Success: 31.25% | ValidRate: 32.86%
2026-01-19 09:17:52 | INFO | game_rl_training | Step 3/1000: Collecting rollouts from 16 episodes...
2026-01-19 09:18:47 | INFO | game_rl_training | Collected 16 valid samples from 33 episodes. Games with valid samples: {'liars_dice': 16}
2026-01-19 09:18:47 | INFO | game_rl_training | Training with 16 samples...
2026-01-19 09:18:54 | INFO | game_rl_training | Step 3 | Reward: 0.3750 | Score: 0.3750 | Success: 37.50% | ValidRate: 32.29%
2026-01-19 09:18:54 | INFO | game_rl_training | Step 4/1000: Collecting rollouts from 16 episodes...
2026-01-19 09:20:10 | INFO | game_rl_training | Collected 16 valid samples from 31 episodes. Games with valid samples: {'liars_dice': 16}
2026-01-19 09:20:10 | INFO | game_rl_training | Training with 16 samples...
2026-01-19 09:20:16 | INFO | game_rl_training | Step 4 | Reward: 0.4994 | Score: 0.5000 | Success: 50.00% | ValidRate: 27.78%
2026-01-19 09:20:16 | INFO | game_rl_training | Step 5/1000: Collecting rollouts from 16 episodes...
2026-01-19 09:21:10 | INFO | game_rl_training | Collected 16 valid samples from 27 episodes. Games with valid samples: {'liars_dice': 16}
2026-01-19 09:21:10 | INFO | game_rl_training | Training with 16 samples...
2026-01-19 09:21:17 | INFO | game_rl_training | Step 5 | Reward: 0.3750 | Score: 0.3750 | Success: 37.50% | ValidRate: 22.95%
2026-01-19 09:21:17 | INFO | game_rl_training | Step 6/1000: Collecting rollouts from 16 episodes...
2026-01-19 09:22:25 | INFO | game_rl_training | Collected 16 valid samples from 37 episodes. Games with valid samples: {'liars_dice': 16}
2026-01-19 09:22:25 | INFO | game_rl_training | Training with 16 samples...
2026-01-19 09:22:31 | INFO | game_rl_training | Step 6 | Reward: 0.5625 | Score: 0.5625 | Success: 56.25% | ValidRate: 22.22%
2026-01-19 09:22:31 | INFO | game_rl_training | Step 7/1000: Collecting rollouts from 16 episodes...
2026-01-19 09:23:28 | INFO | game_rl_training | Collected 16 valid samples from 31 episodes. Games with valid samples: {'liars_dice': 16}
2026-01-19 09:23:28 | INFO | game_rl_training | Training with 16 samples...
2026-01-19 09:23:33 | INFO | game_rl_training | Step 7 | Reward: 0.6862 | Score: 0.6875 | Success: 68.75% | ValidRate: 27.78%
2026-01-19 09:23:33 | INFO | game_rl_training | Step 8/1000: Collecting rollouts from 16 episodes...
