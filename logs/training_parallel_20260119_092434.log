2026-01-19 09:24:39 | INFO | game_rl_training | Setting up PPO+LoRA training...
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: tech-whiz0314 (tech-whiz0314-beta) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /root/workspace/game_rl_training/wandb/run-20260119_092440-6a4nik68
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-planet-78
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tech-whiz0314-beta/game-rl-training
wandb: üöÄ View run at https://wandb.ai/tech-whiz0314-beta/game-rl-training/runs/6a4nik68
2026-01-19 09:24:41 | INFO | game_rl_training | Loading tokenizer: Qwen/Qwen3-4B-Instruct-2507
2026-01-19 09:24:42 | INFO | game_rl_training | Loading base model: Qwen/Qwen3-4B-Instruct-2507
`torch_dtype` is deprecated! Use `dtype` instead!
2026-01-19 09:24:45 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:02<00:04,  2.30s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:04<00:02,  2.45s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.64s/it]
2026-01-19 09:24:50 | INFO | game_rl_training | Applying LoRA configuration...
2026-01-19 09:24:50 | WARNING | root | The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.
2026-01-19 09:24:51 | INFO | root | peft adapter initialised
2026-01-19 09:24:51 | INFO | game_rl_training | Creating reference model...
2026-01-19 09:24:51 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:02<00:04,  2.13s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:04<00:02,  2.31s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.55s/it]
2026-01-19 09:24:56 | WARNING | root | A <class 'transformers.models.qwen3.modeling_qwen3.Qwen3ForCausalLM'> model is loaded from 'Qwen/Qwen3-4B-Instruct-2507', and no v_head weight is found. This IS expected if you are not resuming PPO training.
2026-01-19 09:24:57 | INFO | game_rl_training | Initializing PPO trainer...
wandb: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
/root/workspace/game_rl_training/venv/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:262: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.
  warnings.warn(
2026-01-19 09:24:57 | INFO | game_rl_training | Using local OpenSpiel execution (in-process inference, no env server, no vLLM).
2026-01-19 09:24:57 | INFO | game_rl_training | Initializing curriculum sampler...
2026-01-19 09:24:57 | INFO | game_rl_training | Setup complete!
2026-01-19 09:24:57 | INFO | game_rl_training | Running SFT warmup for 100 steps...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/root/workspace/game_rl_training/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
2026-01-19 09:25:02 | INFO | game_rl_training | SFT step 10/100 | Loss: 3.0091
2026-01-19 09:25:06 | INFO | game_rl_training | SFT step 20/100 | Loss: 3.6131
2026-01-19 09:25:10 | INFO | game_rl_training | SFT step 30/100 | Loss: 3.1308
2026-01-19 09:25:13 | INFO | game_rl_training | SFT step 40/100 | Loss: 2.9200
2026-01-19 09:25:16 | INFO | game_rl_training | SFT step 50/100 | Loss: 2.6181
2026-01-19 09:25:21 | INFO | game_rl_training | SFT step 60/100 | Loss: 2.4435
2026-01-19 09:25:25 | INFO | game_rl_training | SFT step 70/100 | Loss: 2.2206
2026-01-19 09:25:29 | INFO | game_rl_training | SFT step 80/100 | Loss: 2.0018
2026-01-19 09:25:36 | INFO | game_rl_training | SFT step 100/100 | Loss: 2.0378
2026-01-19 09:25:36 | INFO | game_rl_training | SFT warmup complete. Avg loss: 2.0378
2026-01-19 09:25:36 | INFO | game_rl_training | Starting PPO training...
2026-01-19 09:25:36 | INFO | game_rl_training | Step 1/1000: Collecting rollouts from 16 episodes...
2026-01-19 09:25:36 | INFO | game_rl_training | Running 48 episodes with 8 workers...
Caching is incompatible with gradient checkpointing in Qwen3DecoderLayer. Setting `past_key_values=None`.
/root/workspace/game_rl_training/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
2026-01-19 09:26:36 | INFO | game_rl_training | Parallel collection complete: 16 samples from 26 episodes. Games: {'liars_dice': 16}
2026-01-19 09:26:36 | INFO | game_rl_training | Training with 16 samples...
2026-01-19 09:26:42 | INFO | game_rl_training | Step 1 | Reward: 0.0000 | Score: 0.0000 | Success: 0.00% | ValidRate: 51.92%
2026-01-19 09:26:42 | INFO | game_rl_training | Step 2/1000: Collecting rollouts from 16 episodes...
2026-01-19 09:26:42 | INFO | game_rl_training | Running 48 episodes with 8 workers...
2026-01-19 09:27:41 | INFO | game_rl_training | Parallel collection complete: 16 samples from 27 episodes. Games: {'liars_dice': 16}
2026-01-19 09:27:41 | INFO | game_rl_training | Training with 16 samples...
2026-01-19 09:27:47 | INFO | game_rl_training | Step 2 | Reward: 0.0625 | Score: 0.0625 | Success: 6.25% | ValidRate: 45.06%
2026-01-19 09:27:47 | INFO | game_rl_training | Step 3/1000: Collecting rollouts from 16 episodes...
2026-01-19 09:27:47 | INFO | game_rl_training | Running 48 episodes with 8 workers...
